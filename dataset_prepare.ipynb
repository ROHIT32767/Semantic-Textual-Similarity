{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (3.5.0)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7cd2d86faae0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/datasets/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7cd2d84e9e50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/datasets/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7cd2d84ea360>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/datasets/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7cd2d84ea600>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/datasets/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7cd2d84ea7e0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/datasets/\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: filelock in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/abhinav/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "^C\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sts benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 25.77ba/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 76.20ba/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 66.09ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "259728"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"mteb/stsbenchmark-sts\")\n",
    "\n",
    "ds['train'].to_csv('./datasets/raw/sts-train.csv')\n",
    "ds['test'].to_csv('./datasets/raw/sts-test.csv')\n",
    "ds['validation'].to_csv('./datasets/raw/sts-val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_type in ['train', 'test', 'val']:\n",
    "    df = pd.read_csv(f'./datasets/raw/sts-{dataset_type}.csv')\n",
    "    df.drop(columns=['split', 'genre', 'dataset', 'year', 'sid'], axis=1, inplace=True)\n",
    "    df['s1'] = df['sentence1']\n",
    "    df['s2'] = df['sentence2']\n",
    "    df.drop(columns=['sentence1', 'sentence2'], axis=1, inplace=True)\n",
    "    df['score'] = df['score'].astype(float)*0.2 # Scale between 0 and 1\n",
    "    df = df[['s1', 's2', 'score']]\n",
    "    df.insert(0, 'id', [f'sts_{dataset_type}_' + str(i) for i in range(1, len(df) + 1)])\n",
    "    df.to_csv(f'./datasets/final/sts-{dataset_type}.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Question Pairs Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/raw/quora_questions.csv')\n",
    "df.drop(columns=['id', 'qid1', 'qid2'], axis=1, inplace=True)\n",
    "df['s1'] = df['question1']\n",
    "df['s2'] = df['question2']\n",
    "df.drop(columns=['question1', 'question2'], axis=1, inplace=True)\n",
    "df['score'] = df['is_duplicate'].astype(float)\n",
    "df = df[['s1', 's2', 'score']]\n",
    "\n",
    "dataframes = { 'train': None, 'test': None, 'val': None}\n",
    "\n",
    "dataframes['train'], dataframes['test'] = train_test_split(df, test_size=0.2, random_state=42)\n",
    "dataframes['train'], dataframes['val'] = train_test_split(dataframes['train'], test_size=0.1, random_state=42)\n",
    "for dataset_type in ['train', 'test', 'val']:\n",
    "    dataframes[dataset_type].insert(0, 'id', [f'quora_{dataset_type}_' + str(i) for i in range(1, len(dataframes[dataset_type]) + 1)])\n",
    "    dataframes[dataset_type].to_csv(f'./datasets/final/quora-{dataset_type}.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SICK Dataset\n",
    "\n",
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./datasets/raw/SICK.txt', sep='\\t')\n",
    "\n",
    "df = df.rename(columns={\n",
    "    # 'pair_ID': 'id',\n",
    "    'sentence_A': 's1',\n",
    "    'sentence_B': 's2',\n",
    "    'relatedness_score': 'score',\n",
    "})\n",
    "\n",
    "df['score'] = df['score'] / 5.0\n",
    "\n",
    "unified_columns = [\n",
    "    's1', 's2', 'score'\n",
    "]\n",
    "\n",
    "df = df[unified_columns]\n",
    "\n",
    "dataframes = { 'train': None, 'test': None, 'val': None}\n",
    "\n",
    "dataframes['train'], dataframes['test'] = train_test_split(df, test_size=0.2, random_state=42)\n",
    "dataframes['train'], dataframes['val'] = train_test_split(dataframes['train'], test_size=0.1, random_state=42)\n",
    "for dataset_type in ['train', 'test', 'val']:\n",
    "    dataframes[dataset_type].insert(0, 'id', [f'sick_{dataset_type}_' + str(i) for i in range(1, len(dataframes[dataset_type]) + 1)])\n",
    "    dataframes[dataset_type].to_csv(f'./datasets/final/sick-{dataset_type}.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ms_paraphrase(filepath):\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        lines = lines[1:]\n",
    "        for line in lines:\n",
    "            if not line.strip() or line.startswith('//'):\n",
    "                continue\n",
    "                \n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 5:\n",
    "                quality = parts[0]\n",
    "                id1 = parts[1]\n",
    "                id2 = parts[2]\n",
    "                string1 = '\\t'.join(parts[3:-1])\n",
    "                string2 = parts[-1]\n",
    "                data.append([quality, id1, id2, string1, string2])\n",
    "    return pd.DataFrame(data, columns=['Quality', '#1 ID', '#2 ID', '#1 String', '#2 String'])\n",
    "\n",
    "\n",
    "def transform_msr(df, dataset_type):\n",
    "    df.drop(columns=['#1 ID', '#2 ID'], axis=1, inplace=True)\n",
    "    df['s1'] = df['#1 String']\n",
    "    df['s2'] = df['#2 String']\n",
    "    df.drop(columns=['#1 String', '#2 String'], axis=1, inplace=True)\n",
    "    df['score'] = df['Quality'].astype(float)\n",
    "    df = df[['s1', 's2', 'score']]\n",
    "    df.insert(0, 'id', [f'msr_{dataset_type}_' + str(i) for i in range(1, len(df) + 1)])\n",
    "    df.to_csv(f'./datasets/final/msr-{dataset_type}.csv', index=False, header=True)\n",
    "\n",
    "train_df = load_ms_paraphrase('datasets/raw/msr_train.txt')\n",
    "test_df = load_ms_paraphrase('datasets/raw/msr_test.txt')\n",
    "\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "transform_msr(train_df, 'train')\n",
    "transform_msr(test_df, 'test')\n",
    "transform_msr(val_df, 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/final/sts-train.csv \t:\t 5749\n",
      "datasets/final/sts-test.csv \t:\t 1379\n",
      "datasets/final/sts-val.csv \t:\t 1500\n",
      "datasets/final/quora-train.csv \t:\t 291132\n",
      "datasets/final/quora-test.csv \t:\t 80871\n",
      "datasets/final/quora-val.csv \t:\t 32348\n",
      "datasets/final/msr-train.csv \t:\t 3260\n",
      "datasets/final/msr-test.csv \t:\t 1725\n",
      "datasets/final/msr-val.csv \t:\t 816\n",
      "datasets/final/sick-train.csv \t:\t 7084\n",
      "datasets/final/sick-test.csv \t:\t 1968\n",
      "datasets/final/sick-val.csv \t:\t 788\n"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "    'datasets/final/sts-train.csv',\n",
    "    'datasets/final/sts-test.csv',\n",
    "    'datasets/final/sts-val.csv',\n",
    "    'datasets/final/quora-train.csv',\n",
    "    'datasets/final/quora-test.csv',\n",
    "    'datasets/final/quora-val.csv',\n",
    "    'datasets/final/msr-train.csv',\n",
    "    'datasets/final/msr-test.csv',\n",
    "    'datasets/final/msr-val.csv',\n",
    "    'datasets/final/sick-train.csv',\n",
    "    'datasets/final/sick-test.csv',\n",
    "    'datasets/final/sick-val.csv',\n",
    "]\n",
    "\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(dataset)\n",
    "    print(dataset, '\\t:\\t', df.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(dataset1_path, dataset2_path, output_path):\n",
    "    df1 = pd.read_csv(dataset1_path)\n",
    "    df2 = pd.read_csv(dataset2_path)\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "    ## Shuffle the datasets\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    df.to_csv(output_path, index=False, header=True)\n",
    "    \n",
    "merge_datasets('datasets/final/sts-train.csv', 'datasets/final/sick-train.csv', 'train.csv')\n",
    "merge_datasets('datasets/final/sts-test.csv', 'datasets/final/sick-test.csv', 'test.csv')\n",
    "merge_datasets('datasets/final/sts-val.csv', 'datasets/final/sick-val.csv', 'val.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
