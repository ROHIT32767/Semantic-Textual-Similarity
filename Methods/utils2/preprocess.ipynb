{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-09T10:28:46.069866Z","iopub.status.busy":"2024-04-09T10:28:46.068971Z","iopub.status.idle":"2024-04-09T10:28:48.815093Z","shell.execute_reply":"2024-04-09T10:28:48.814089Z","shell.execute_reply.started":"2024-04-09T10:28:46.069831Z"},"trusted":true},"outputs":[],"source":["import re\n","import sys\n","import nltk\n","import json\n","import warnings\n","import numpy as np\n","import pandas as pd\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import PorterStemmer\n","\n","warnings.filterwarnings('ignore')\n","np.set_printoptions(threshold=sys.maxsize)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T10:28:48.817081Z","iopub.status.busy":"2024-04-09T10:28:48.816699Z","iopub.status.idle":"2024-04-09T10:28:48.938986Z","shell.execute_reply":"2024-04-09T10:28:48.937720Z","shell.execute_reply.started":"2024-04-09T10:28:48.817057Z"},"trusted":true},"outputs":[],"source":["# Load all the data\n","trainpath = '../../train.csv'\n","testpath = '../../test.csv'\n","valpath = '../../val.csv'\n","\n","traindata = pd.read_csv(trainpath)\n","testdata = pd.read_csv(testpath)\n","valdata = pd.read_csv(valpath)\n","\n","traindata = traindata[['id', 's1', 's2', 'score']]\n","testdata = testdata[['id', 's1', 's2', 'score']]\n","valdata = valdata[['id', 's1', 's2', 'score']]"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T10:28:48.941967Z","iopub.status.busy":"2024-04-09T10:28:48.941184Z","iopub.status.idle":"2024-04-09T10:28:48.949346Z","shell.execute_reply":"2024-04-09T10:28:48.948387Z","shell.execute_reply.started":"2024-04-09T10:28:48.941931Z"},"trusted":true},"outputs":[],"source":["def remove_punctuation(text):\n","    return re.sub(r'[^\\w\\s]', '', text)\n","\n","def remove_number(text):\n","    return re.sub(r'\\d+', 'num', text)\n","\n","def replace_url(text):\n","    return re.sub(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', 'url', text)\n","\n","def replace_hashtags(text):\n","    return re.sub(r'#[a-zA-Z\\d]+', 'hashtag', text)\n","\n","def replace_email(text):\n","    return re.sub(r'[a-zA-Z\\.]+@[a-zA-Z\\.\\d]+', 'email', text)\n","\n","def replace_mentions(text):\n","    return re.sub(r'@[a-zA-Z\\.\\d_]+', 'mention', text)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T10:28:48.952245Z","iopub.status.busy":"2024-04-09T10:28:48.951857Z","iopub.status.idle":"2024-04-09T10:28:48.964017Z","shell.execute_reply":"2024-04-09T10:28:48.963144Z","shell.execute_reply.started":"2024-04-09T10:28:48.952215Z"},"trusted":true},"outputs":[],"source":["# Stop words, Lemmatization and Stemming\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","stemmer = PorterStemmer()"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T10:28:48.965752Z","iopub.status.busy":"2024-04-09T10:28:48.965421Z","iopub.status.idle":"2024-04-09T10:28:48.973693Z","shell.execute_reply":"2024-04-09T10:28:48.972771Z","shell.execute_reply.started":"2024-04-09T10:28:48.965728Z"},"trusted":true},"outputs":[],"source":["def preprocess_text(text):\n","    text = remove_punctuation(text)\n","    text = remove_number(text)\n","    text = replace_url(text)\n","    text = replace_hashtags(text)\n","    text = replace_email(text)\n","    text = replace_mentions(text)\n","    # convert to lower case\n","    text = text.lower()\n","    sentence = text.split()\n","    # remove stop words\n","    sentence = [word for word in sentence if word not in stop_words]\n","    # apply lemmatize\n","    sentence = [lemmatizer.lemmatize(word) for word in sentence]\n","    # apply stemming\n","    sentence = [stemmer.stem(word) for word in sentence]\n","    return sentence\n","\n","def process_data(data):\n","    data['s1'] = data['s1'].apply(lambda x: preprocess_text(x))\n","    data['s2'] = data['s2'].apply(lambda x: preprocess_text(x))\n","    return data\n","    \n","def get_vocab(data):\n","    vocab = set()\n","    for _, row in data.iterrows():\n","        for word in row['s1']:\n","            vocab.add(word)\n","        for word in row['s2']:\n","            vocab.add(word)\n","    return vocab"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T10:28:48.975244Z","iopub.status.busy":"2024-04-09T10:28:48.974793Z","iopub.status.idle":"2024-04-09T10:28:49.521666Z","shell.execute_reply":"2024-04-09T10:28:49.520455Z","shell.execute_reply.started":"2024-04-09T10:28:48.975221Z"},"trusted":true},"outputs":[],"source":["traindata = process_data(traindata)\n","testdata = process_data(testdata)\n","valdata = process_data(valdata)\n","\n","# Create vocab and word2idx\n","vocab = get_vocab(traindata)\n","word2idx = {word: idx for idx, word in enumerate(sorted(vocab))}\n","\n","file_path = 'word2idx.json'\n","file_path = 'data/word2idx.json'\n","\n","# # Save the dictionary to a JSON file\n","with open(file_path, 'w') as file:\n","    json.dump(word2idx, file)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T10:28:49.523116Z","iopub.status.busy":"2024-04-09T10:28:49.522861Z","iopub.status.idle":"2024-04-09T10:28:49.576470Z","shell.execute_reply":"2024-04-09T10:28:49.575827Z","shell.execute_reply.started":"2024-04-09T10:28:49.523095Z"},"trusted":true},"outputs":[],"source":["# Store the processed datasets\n","traindata.to_csv('data/train.csv', index=False)\n","testdata.to_csv('data/test.csv', index=False)\n","valdata.to_csv('data/val.csv', index=False)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T10:28:49.577785Z","iopub.status.busy":"2024-04-09T10:28:49.577518Z","iopub.status.idle":"2024-04-09T10:28:49.581749Z","shell.execute_reply":"2024-04-09T10:28:49.580976Z","shell.execute_reply.started":"2024-04-09T10:28:49.577764Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["8597\n"]}],"source":["print(len(vocab))"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T10:28:49.583311Z","iopub.status.busy":"2024-04-09T10:28:49.582692Z","iopub.status.idle":"2024-04-09T10:28:49.599157Z","shell.execute_reply":"2024-04-09T10:28:49.598470Z","shell.execute_reply.started":"2024-04-09T10:28:49.583286Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["                id                                                 s1  \\\n","0   sts_train_4024            [sudan, block, youtub, antiislam, film]   \n","1   sts_train_1409                           [man, ride, white, hors]   \n","2   sts_train_3397  [mr, mors, charg, assault, mr, darvish, charg,...   \n","3   sts_train_1532                    [girl, play, pile, color, ball]   \n","4  sick_train_1874           [person, black, jacket, trick, motorbik]   \n","\n","                                                  s2  score  \n","0  [pakistan, pm, order, youtub, halt, antiislam,...   0.56  \n","1                         [woman, lead, white, hors]   0.36  \n","2  [partner, bijan, darvish, charg, file, fals, p...   0.55  \n","3              [littl, girl, play, pit, color, ball]   1.00  \n","4              [man, black, jacket, trick, motorbik]   0.98  \n"]}],"source":["print(traindata[:5])"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4759065,"sourceId":8066474,"sourceType":"datasetVersion"}],"dockerImageVersionId":30684,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
